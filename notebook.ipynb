{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeitsschritte\n",
    "\n",
    "Was ich erwarte:\n",
    "* Daten laden ###\n",
    "* Erste “Insights”, d.h. explorative Datenanalayse inkl. einiger Charts (Verteilungen, erste Korrelationen etc.,) um die Daten zu verstehen ###\n",
    "* Datacleaning ###\n",
    "* Feature Engineering ###\n",
    "* Modellbildung\n",
    "* alle klassischen Modelle, die Sie bisher kennen\n",
    "* XGBoost ###\n",
    "* Neuronales Netz ###\n",
    "* Für die besten Modelle: Hyperparameter Tuning (GridSearchCV) ###\n",
    "* ggf. auch Mischung verschiedener Modelle (Ensemble) ###\n",
    "* Feature Importance / Permutation Importance ###\n",
    "* Eine Submission soll für die test.csv erstellt werden und bei Kaggle hochgeladen werden\n",
    "* Am Ende zwei reißerische Grafiken mit jeweils einer Einsicht auf Basis der KI-Modelle (storytelling beachten!)\n",
    "* Für jedes Kapitel eine kurze (gerne stichwortartige) Zusammenfassung im Notebook, was gemacht bzw. was noch so versucht wurde, jedoch nicht geklappt hat und welche Erkentnisse im Kapitel gesammelt wurden.\n",
    "\n",
    "\n",
    "Organisatorisches:\n",
    "\n",
    "* Abgabe\n",
    "* Form eines Git-Links zu Ihrem Projekt\n",
    "* Montag 08.04.. , 23:59 Uhr\n",
    "* Sie haben in meinem Unterricht zwei Termine Zeit für diesen Teil (12.02. und 19.02.)\n",
    "\n",
    "# Mies viele Kommentare\n",
    "\n",
    "# One Hot Encoden ??? markdownzelle mit begründung warum nicht verwendet\n",
    "\n",
    "# auf rmspe score bei ML umstellen\n",
    "\n",
    "# in best p.py grobe idee erklären und ablauf\n",
    "\n",
    "# device wahl für torch automatisch\n",
    "\n",
    "# bei prediction mit NN prediction mit e^x verrechnen wegen log(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Laden und zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from best_params import make # importiert die Funktion make aus der Datei best_params.py um die besten Parameter für die ML-Verfahren zu finden\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import xgboost\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "store_df = pd.read_csv(\"store.csv\")\n",
    "\n",
    "# Zusatzinformation der einzelnen Geschäfte in den Haupt DataFrame laden\n",
    "big_df = df.merge(store_df, on=\"Store\", how=\"left\")\n",
    "store_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot 1: Sales by Day of WeekVerkäufe nach Wochentag \n",
    "s_b_d = big_df[\"Sales\"].groupby(big_df[\"DayOfWeek\"]).sum()\n",
    "axs[0, 0].plot(s_b_d)\n",
    "axs[0, 0].set_title('Sales by Day of Week')\n",
    "\n",
    "# Plot 2: Punktwolke mit Vergleich zwischen Customers und Sales \n",
    "axs[0, 1].scatter(big_df[\"Customers\"], big_df[\"Sales\"])\n",
    "axs[0, 1].set_title('Customers vs Sales')\n",
    "\n",
    "# Plot 3: Verkäufe nach Geschäftsart \n",
    "sales_by_storeType = big_df[\"Sales\"].groupby(big_df[\"StoreType\"]).sum()\n",
    "axs[1, 0].plot(sales_by_storeType)\n",
    "axs[1, 0].set_title('Sales by Store Type')\n",
    "\n",
    "# Plot 4: Anzahl der Geschäfte nach Typ \n",
    "amt_stores_by_type = big_df[\"StoreType\"].value_counts()\n",
    "axs[1, 1].bar([\"a\", \"b\", \"c\", \"d\"], amt_stores_by_type)\n",
    "axs[1, 1].set_title('Amount of Stores by Type')\n",
    "\n",
    "# Durchschnittliche Käufe pro Kunde\n",
    "sales_per_customer = big_df[\"Sales\"] / big_df[\"Customers\"]\n",
    "big_df[\"sales_per_customer\"] = sales_per_customer\n",
    "\n",
    "# Durchschnittliche Käufe pro Kunde nach Store Type\n",
    "m_sales_customer_by_st = big_df[\"sales_per_customer\"].groupby(big_df[\"StoreType\"]).sum().values.tolist()\n",
    "axs[2, 0].plot(big_df[\"StoreType\"].unique(), m_sales_customer_by_st)\n",
    "axs[2, 0].set_title(\"mean_sales_p_cust_by_stoTyp\")\n",
    "\n",
    "# Grafiken ausgeben\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# \n",
    "numeric_cols = big_df.select_dtypes(include=[np.number])\n",
    "corr = numeric_cols.corr()\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datacleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cleaning(df):\n",
    "    # Leere Werte der Spalte Competition Distance mit 0 füllen\n",
    "    df[\"CompetitionDistance\"] = df[\"CompetitionDistance\"].fillna(-1)\n",
    "    # \n",
    "    comp_zeros = df[\"CompetitionDistance\"] == -1\n",
    "    df[\"CompetitionOpenSinceMonth\"][comp_zeros] = -1\n",
    "    df[\"CompetitionOpenSinceYear\"][comp_zeros] = -1\n",
    "    df = df.dropna(axis=0)\n",
    "    return df\n",
    "\n",
    "# Die Bereinigung auf den Haupt DataFrame anwenden \n",
    "big_df = apply_cleaning(big_df)\n",
    "big_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "big_df[\"Date\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fe(df):\n",
    "    \n",
    "    #\n",
    "    to_drop = [\"Date\", \"Store\"]\n",
    "    df[\"year\"] = df[\"Date\"].str.split(\"-\").str[0]\n",
    "    df[\"month\"] = df[\"Date\"].str.split(\"-\").str[1]\n",
    "    df[\"year\"] = df[\"year\"].astype(int) - 2013\n",
    "    df[\"month\"] = df[\"month\"].astype(int) - 1\n",
    "    \n",
    "    df = df.drop(to_drop, axis=1)\n",
    "\n",
    "    #\n",
    "    df[\"CompetitionDistance\"] = df[\"CompetitionDistance\"].astype(int)\n",
    "    df[\"CompetitionOpenSinceMonth\"] = df[\"CompetitionOpenSinceMonth\"].astype(int)\n",
    "    df[\"CompetitionOpenSinceYear\"] = df[\"CompetitionOpenSinceYear\"].astype(int)\n",
    "    df[\"Promo2SinceWeek\"] = df[\"Promo2SinceWeek\"].astype(int)\n",
    "    df[\"Promo2SinceYear\"] = df[\"Promo2SinceYear\"].astype(int)\n",
    "\n",
    "    # \n",
    "    #big_df[\"promo2week_bool\"] = big_df[\"Promo2SinceWeek\"].isna()\n",
    "\n",
    "    # Die Storytypen und Sortimentenart in eine numerischen Skalierung überführen \n",
    "    store_type_dict = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3}\n",
    "    df[\"StoreType\"] = df[\"StoreType\"].map(store_type_dict)\n",
    "    assortment_dict = {\"a\": 0, \"b\": 1, \"c\": 2}\n",
    "    df[\"Assortment\"] = df[\"Assortment\"].map(assortment_dict)\n",
    "\n",
    "    # Die Spalte Promo Intervall enthält 3 Unique Werte. \n",
    "    # Um zu verhindern, Das eine Wertigkeit erlernt wird, haben wir uns dazu entschieden Für jeden Monat eine Spalte anzulegen.\n",
    "    # Alle Spalten werden bei Erstellung mit Nullen gefüllt. Dann wird über die Elemente der Spalte PromoIntervall itteriert und je nach einem der 3 Fälle, \n",
    "    # die entsprechenden Zellen der entsprechenden Spalten mit Einsen gefüllt (Beim Januar also die Zellen der Spalten Januar April, Juli und Oktober).\n",
    "    df[\"jan\"] = 0\n",
    "    df[\"feb\"] = 0\n",
    "    df[\"mar\"] = 0\n",
    "    df[\"apr\"] = 0\n",
    "    df[\"may\"] = 0\n",
    "    df[\"jun\"] = 0\n",
    "    df[\"jul\"] = 0\n",
    "    df[\"aug\"] = 0\n",
    "    df[\"sep\"] = 0\n",
    "    df[\"oct\"] = 0\n",
    "    df[\"nov\"] = 0\n",
    "    df[\"dec\"] = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        start = row[\"PromoInterval\"].split(\",\")[0]\n",
    "        match start:\n",
    "            case \"Jan\":\n",
    "                df.at[index, \"jan\"] = 1\n",
    "                df.at[index, \"apr\"] = 1\n",
    "                df.at[index, \"jul\"] = 1\n",
    "                df.at[index, \"oct\"] = 1\n",
    "\n",
    "            case \"Feb\":\n",
    "                df.at[index, \"feb\"] = 1\n",
    "                df.at[index, \"may\"] = 1\n",
    "                df.at[index, \"aug\"] = 1\n",
    "                df.at[index, \"nov\"] = 1\n",
    "\n",
    "            case \"Mar\":\n",
    "                df.at[index, \"mar\"] = 1\n",
    "                df.at[index, \"jun\"] = 1\n",
    "                df.at[index, \"sep\"] = 1\n",
    "                df.at[index, \"dec\"] = 1\n",
    "\n",
    "    df = df.drop(\"PromoInterval\", axis=1)   \n",
    "\n",
    "    # Alle Spaltentitel in lower Snake Case umwandeln\n",
    "    for col in df.columns:\n",
    "        col_n = re.sub(r'(?<!^)(?=[A-Z])', '_', col).lower()\n",
    "        df.rename(columns={col: col_n}, inplace=True)\n",
    "\n",
    "    # !!!!!!! mit wertigkeit weil weihnachten für shoppen wichtiger als kein urlaub\n",
    "    holiday_map = {\"0\": 0, \"a\": 1, \"b\": 2, \"c\": 3, 0: 0}\n",
    "    df[\"state_holiday\"] = df[\"state_holiday\"].map(holiday_map)\n",
    "\n",
    "    # Da in diesen Spalten im Vergleich zu den anderen des DataFrames sehr hohe Werte vorkommen, haben wir uns dazu entschlossen, den natürlichen Logarithmus der jeweiligen Spalte zu ziehen, damit die Modelle vernünftig lernen können\n",
    "    df[\"sales\"] = np.log(df[\"sales\"])\n",
    "    df[\"customers\"] = np.log(df[\"customers\"])\n",
    "    df[\"competition_distance\"] = np.log(df[\"competition_distance\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "big_df = do_fe(big_df)\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(big_df[\"PromoInterval\"].unique()) # im 3-Monats Intervall\n",
    "big_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modellbildung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "X = big_df.drop(\"sales\", axis=1)\n",
    "y = big_df[\"sales\"]\n",
    "\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# \n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "#\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X)\n",
    "x_scaled=scaler.transform(X)\n",
    "\n",
    "x_scaled_df = pd.DataFrame(x_scaled, columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled_df, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Die Funktion make kann aufgerufen werden, um die optimalen Parameter für die Machine Learning-Verfahren zu errechnen und in eine json-Datei zu speichern\n",
    "make(X_train, y_train)\n",
    "# Die errechneten Parameter aus der json-Datei laden\n",
    "params_f = json.load(open(\"best_params.json\", \"r\"))\n",
    "\n",
    "def make_voting(X_train, y_train, X_test, y_test):\n",
    "    # Das jeweilige Dictionary mit dem Parametern für das jeweilige Verfahren herausnehmen\n",
    "    best_knn_params = params_f[\"knn\"]\n",
    "    best_tree_params = params_f[\"tree\"]\n",
    "    best_forest_params = params_f[\"forest\"]\n",
    "    best_svm_params = params_f[\"svm\"]\n",
    "    #best_lin_reg_params = params_f[\"lin_reg\"]\n",
    "    best_xg_params = params_f[\"xg\"]\n",
    "\n",
    "    # Je Parameter Dictionary ein Vorhersagemodell aus diesen Daten bauen\n",
    "    knn = KNeighborsRegressor(n_neighbors=best_knn_params[\"kneighborsregressor__n_neighbors\"],\n",
    "                             weights=best_knn_params[\"kneighborsregressor__weights\"],\n",
    "                             algorithm=best_knn_params[\"kneighborsregressor__algorithm\"])\n",
    "    tree = DecisionTreeRegressor(max_depth=best_tree_params[\"decisiontreeregressor__max_depth\"],\n",
    "                                 min_samples_split=best_tree_params[\"decisiontreeregressor__min_samples_split\"],\n",
    "                                 min_samples_leaf=best_tree_params[\"decisiontreeregressor__min_samples_leaf\"])\n",
    "    forest = RandomForestRegressor(n_estimators=best_forest_params[\"randomforestregressor__n_estimators\"],\n",
    "                                   max_depth=best_forest_params[\"randomforestregressor__max_depth\"],\n",
    "                                   min_samples_split=best_forest_params[\"randomforestregressor__min_samples_split\"],\n",
    "                                   min_samples_leaf=best_forest_params[\"randomforestregressor__min_samples_leaf\"])\n",
    "    svm = SVR(kernel=best_svm_params[\"svr__kernel\"],\n",
    "             degree=best_svm_params[\"svr__degree\"],\n",
    "             C=best_svm_params[\"svr__C\"])\n",
    "    #lin_reg = LinearRegression(fit_intercept=best_lin_reg_params[\"linearregression__fit_intercept\"])\n",
    "    xg = xgboost.XGBRegressor(learning_rate=best_xg_params[\"xgbregressor__learning_rate\"],\n",
    "                            max_depth=best_xg_params[\"xgbregressor__max_depth\"],\n",
    "                            n_estimators=best_xg_params[\"xgbregressor__n_estimators\"])\n",
    "\n",
    "    # Die Machine Learning Modelle auf die Daten trainieren und ihren r2-Score ausgeben\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(\"knn done\")\n",
    "    print(mean_squared_error(y_test, knn.predict(X_test)))\n",
    "    tree.fit(X_train, y_train)\n",
    "    print(\"tree done\")\n",
    "    print(mean_squared_error(y_test, tree.predict(X_test)))\n",
    "    forest.fit(X_train, y_train)\n",
    "    print(\"forest done\")\n",
    "    print(mean_squared_error(y_test, forest.predict(X_test)))\n",
    "    svm.fit(X_train, y_train)\n",
    "    print(\"svm done\")\n",
    "    print(mean_squared_error(y_test, svm.predict(X_test)))\n",
    "    #lin_reg.fit(X_train, y_train)\n",
    "    #print(\"lin_reg done\")\n",
    "    #print(mean_squared_error(y_test, lin_reg.predict(X_test)))\n",
    "    xg.fit(X_train, y_train)\n",
    "    print(\"xg done\")\n",
    "    print(mean_squared_error(y_test, xg.predict(X_test)))\n",
    "\n",
    "    # Einen VotingRegressor aus den am besten performenden Modellen bauen und auf die Daten trainieren\n",
    "    voting = VotingRegressor(estimators=[(\"knn\", knn), (\"tree\", tree), (\"forest\", forest), (\"svm\", svm), (\"xg\", xg)], n_jobs=-1)\n",
    "    voting.fit(X_train, y_train)\n",
    "    print(\"voting done\")\n",
    "\n",
    "    # Den Mean Squared Error und die Genauigkeit des Votings ausgeben\n",
    "    error = mean_squared_error(y_test, voting.predict(X_test))\n",
    "    print(\"ERROR:\", error)\n",
    "    accuracy = voting.score(y_test, voting.predict(X_test))\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Die Feature importance vom Voting errechnen und anzeigen\n",
    "    feature_importance = voting.feature_importances_\n",
    "    plt.barh(X_train.columns, feature_importance)\n",
    "    plt.show()\n",
    "\n",
    "    # Die Testdaten laden und auf dieselbe Weise transformieren wie die Trainingsdaten (Bereinigung und Feature Engineering)\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    test_df = apply_cleaning(test_df)\n",
    "    test_df = do_fe(test_df)\n",
    "\n",
    "    ids = [i for i in range(len(test_df[\"id\"]))]\n",
    "    \n",
    "    # Mit jedem Machine Learning Modell aus den Testdaten vorhersagen erstellen\n",
    "    knn_pred = knn.predict(test_df)\n",
    "    tree_pred = tree.predict(test_df)\n",
    "    forest_pred = forest.predict(test_df)\n",
    "    svm_pred = svm.predict(test_df)\n",
    "    xg_pred = xg.predict(test_df)\n",
    "    voting_pred = voting.predict(test_df)\n",
    "\n",
    "    # \n",
    "    knn_df = pd.DataFrame({\"Id\": ids, \"Sales\": knn_pred})\n",
    "    knn_df.to_csv(\"knn.csv\", index=False)\n",
    "\n",
    "    tree_df = pd.DataFrame({\"Id\": ids, \"Sales\": tree_pred})\n",
    "    tree_df.to_csv(\"tree.csv\", index=False)\n",
    "\n",
    "    forest_df = pd.DataFrame({\"Id\": ids, \"Sales\": forest_pred})\n",
    "    forest_df.to_csv(\"forest.csv\", index=False)\n",
    "\n",
    "    svm_df = pd.DataFrame({\"Id\": ids, \"Sales\": svm_pred})\n",
    "    svm_df.to_csv(\"svm.csv\", index=False)\n",
    "\n",
    "    xg_df = pd.DataFrame({\"Id\": ids, \"Sales\": xg_pred})\n",
    "    xg_df.to_csv(\"xg.csv\", index=False)\n",
    "\n",
    "    v_df = pd.DataFrame({\"Id\": ids, \"Sales\": voting_pred})\n",
    "    v_df.to_csv(\"voting-csv\", index=False)\n",
    "    \n",
    "    return voting\n",
    "\n",
    "vot = make_voting(X_train, y_train, X_test, y_test) # error: accuracy = voting.score(y_test, voting.predict(X_test)) -> Expected 2D array, got 1D array instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1_h = 500\n",
    "        self.l2_h = 500\n",
    "        self.l3_h = 200\n",
    "        self.l4_h = 1\n",
    "\n",
    "        # Für Linear-Layer entschieden, da es in diesem Fall um Zusammenhänge je Zeile geht\n",
    "        self.l1 = nn.Linear(X.shape[1], self.l1_h)\n",
    "        self.l2 = nn.Linear(self.l1_h, self.l2_h)\n",
    "        self.l3 = nn.Linear(self.l2_h, self.l3_h)\n",
    "        self.l4 = nn.Linear(self.l3_h, self.l4_h)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Batch Normalisierung nach jeder Layer, um extrem große Werte zu verhindern  \n",
    "        self.b1 = nn.BatchNorm1d(self.l1_h)\n",
    "        self.b2 = nn.BatchNorm1d(self.l2_h)\n",
    "        self.b3 = nn.BatchNorm1d(self.l3_h)\n",
    "        self.b4 = nn.BatchNorm1d(self.l4_h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.b1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.b4(x)\n",
    "        return x\n",
    "        \n",
    "# \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "# \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = torch.tensor(x.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]\n",
    "\n",
    "# Manuel Seed gesetzt, damit die Ergebnisse der verschiedenen Durchläufe des neuronalen Netzes besser vergleichbar werden \n",
    "torch.manual_seed(1234)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using: \", device)\n",
    "\n",
    "# \n",
    "model = Model()\n",
    "model.to(device)\n",
    "optmizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "dataset = Dataset(X_train, y_train)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "#schedular = torch.optim.lr_scheduler.CosineAnnealingLR(optmizer, T_max=20)\n",
    "test_loader = torch.utils.data.DataLoader(Dataset(X_test, y_test), batch_size=256, shuffle=True)\n",
    "\n",
    "epochs = 60\n",
    "losses = []\n",
    "test_losses = []\n",
    "\n",
    "# \n",
    "for epoch in range(epochs):\n",
    "   for xb, yb in data_loader:\n",
    "       xb, yb = xb.to(device), yb.to(device)\n",
    "       y_hat = model(xb)\n",
    "       loss = loss_fn(y_hat, yb)\n",
    "       loss.backward()\n",
    "       optmizer.step()\n",
    "       optmizer.zero_grad()\n",
    "       #schedular.step()\n",
    "   print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}\")\n",
    "   losses.append(loss.item())\n",
    "   with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "              xb = xb.to(device)\n",
    "              y_hat = model(xb)\n",
    "              test_loss = loss_fn(y_hat, yb)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "# Einen Zwischenstand des Modells mit den errechneten Gewichten abspeichern\n",
    "torch.save(model.state_dict(), \"model_small_XXX.pt\")\n",
    "\n",
    "# Die beiden Listen Train- und Testloss als Grafen mit beschrifteten Achsen ausgeben \n",
    "plt.plot(losses, label=\"train_loss\")\n",
    "plt.plot(test_losses, label=\"test_loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "# Bei aktuellen Einstellungen verringert sich Der Loss des Modells über 60 Epochen auf ungefähr 0,1. Dabei weichen Train und Testlos kaum voneinander ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "nn_res = []\n",
    "final_test_df = pd.read_csv(\"test.csv\")\n",
    "final_test_df = apply_cleaning(final_test_df)\n",
    "final_test_df = do_fe(final_test_df)\n",
    "final_test_dataset = TestDataset(final_test_df)\n",
    "\n",
    "# Das Modell mit den Gewichten aus dem Zwischenstand laden\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"model_small_60E_loss0point2.pt\"))\n",
    "\n",
    "# Die Vorhersagen des Modells auf die Testdaten erstellen\n",
    "for xb in final_test_dataset:\n",
    "    y_hat = model(xb)\n",
    "    nn_res.append(y_hat.item())\n",
    "\n",
    "# \n",
    "nn_df = pd.DataFrame({\"Id\": [i for i in range(len(final_test_df[\"id\"]))], \"Sales\": nn_res})\n",
    "nn_df.to_csv(\"nn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafiken erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorläufige Ideen für Grafiken:\n",
    "#   - An welchem Wochentag je intervall wie viel gekauft wurde\n",
    "#   - An welchem Wochentag je StoreType wie viel gekauft wurde\n",
    "#   - vorhersage von sales\n",
    "#   - vorhersage von sales per customer\n",
    "#   - monatliche summierte verkaufszahlen in den Jahren 2013 und 2014, 2015 solange noch daten da sind (07), ab 08 vorhersagen, Monate mit Promo-Intervall hinterlegen mit Box\n",
    "#   - Punktwolke: x-Achse: Entfernungen min bis max., y-Achse: Verkäufe jedes Shops, farblich unterschieden nach Storetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: die monatlichen Verkaufszahlen der Jahre 2013, 2014 und teilweise 2015 (restliche Werte für 2015 werden mit dem Neuronalen Netz und den Daten der test.csv errechnet). Die Monate, in denen ein Promo-Intervall stattfand, sind farblich hervorgehoben\n",
    "nums13 = df[\"Sales\"][df[\"year\"] == \"2013\"].groupby(df[\"month\"]).sum().values\n",
    "nums14 = df[\"Sales\"][df[\"year\"] == \"2014\"].groupby(df[\"month\"]).sum().values\n",
    "nums15 = df[\"Sales\"][df[\"year\"] == \"2015\"].groupby(df[\"month\"]).sum().values\n",
    "\n",
    "x_elems = [i for i in range(1, 13)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(x_elems, nums13, label=\"2013\")\n",
    "ax.bar(x_elems, nums14, label=\"2014\")\n",
    "ax.bar(x_elems, nums15, label=\"2015\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Scatter-plot mit Entfernungen der Konkurrenzgeschäfte auf der X-Achse und den Verkäufen der jeweiligen Geschäfte auf der Y-Achse. Die Punkte sind farblich nach StoreType unterschieden\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(big_df[\"competition_distance\"], big_df[\"sales\"], c=big_df[\"store_type\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa22b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
