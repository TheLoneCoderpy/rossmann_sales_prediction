{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeitsschritte\n",
    "* Daten laden +\n",
    "* EDA +\n",
    "* Cleaning +\n",
    "* Feature Engineering\n",
    "* Modelle bauen (ML und NN)\n",
    "* Hyperparam. tuning für besten Modelle\n",
    "* Ensemble für Modelle\n",
    "* Feature Importance / Permutation Importance\n",
    "* Submission auf kaggle (+ Screenshot)\n",
    "* 2 geile Grafiken\n",
    "* je Kapitel eine Zusammenfassung\n",
    "\n",
    "Siehe itslearning Aufgabe\n",
    "\n",
    "# Mies viele Kommentare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from best_params import make\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from cuml.ensemble import RandomForestRegressor\n",
    "from cuml.neighbors import KNeighborsRegressor\n",
    "from cuml.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "store_df = pd.read_csv(\"store.csv\")\n",
    "\n",
    "big_df = df.merge(store_df, on=\"Store\", how=\"left\")\n",
    "store_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot 1: Sales by Day of Week\n",
    "s_b_d = big_df[\"Sales\"].groupby(big_df[\"DayOfWeek\"]).sum()\n",
    "axs[0, 0].plot(s_b_d)\n",
    "axs[0, 0].set_title('Sales by Day of Week')\n",
    "\n",
    "# Plot 2: Scatter plot of Customers vs Sales\n",
    "axs[0, 1].scatter(big_df[\"Customers\"], big_df[\"Sales\"])\n",
    "axs[0, 1].set_title('Customers vs Sales')\n",
    "\n",
    "# Plot 3: Sales by Store Type\n",
    "sales_by_storeType = big_df[\"Sales\"].groupby(big_df[\"StoreType\"]).sum()\n",
    "axs[1, 0].plot(sales_by_storeType)\n",
    "axs[1, 0].set_title('Sales by Store Type')\n",
    "\n",
    "# Plot 4: Amount of Stores by Type\n",
    "amt_stores_by_type = big_df[\"StoreType\"].value_counts()\n",
    "axs[1, 1].bar([\"a\", \"b\", \"c\", \"d\"], amt_stores_by_type)\n",
    "axs[1, 1].set_title('Amount of Stores by Type')\n",
    "\n",
    "# mean sales per customer\n",
    "sales_per_customer = big_df[\"Sales\"] / big_df[\"Customers\"]\n",
    "big_df[\"sales_per_customer\"] = sales_per_customer\n",
    "\n",
    "m_sales_customer_by_st = big_df[\"sales_per_customer\"].groupby(big_df[\"StoreType\"]).sum().values.tolist()\n",
    "axs[2, 0].plot(big_df[\"StoreType\"].unique(), m_sales_customer_by_st)\n",
    "axs[2, 0].set_title(\"mean_sales_p_cust_by_stoTyp\")\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datacleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[\"CompetitionDistance\"] = big_df[\"CompetitionDistance\"].fillna(0)\n",
    "\n",
    "comp_zeros = big_df[\"CompetitionDistance\"] == 0\n",
    "\n",
    "big_df[\"CompetitionOpenSinceMonth\"][comp_zeros] = 0\n",
    "big_df[\"CompetitionOpenSinceYear\"][comp_zeros] = 0\n",
    "\n",
    "big_df = big_df.dropna(axis=0)\n",
    "big_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big_df[\"year\"] = big_df[\"Date\"].str.split(\"-\").str[0].astype(int)\n",
    "#big_df[\"month\"] = big_df[\"Date\"].str.split(\"-\").str[1].astype(int)\n",
    "#big_df[\"day\"] = big_df[\"Date\"].str.split(\"-\").str[2].astype(int)\n",
    "\n",
    "to_drop = [\"Date\", \"Store\"]\n",
    "\n",
    "big_df = big_df.drop(to_drop, axis=1)\n",
    "\n",
    "big_df[\"CompetitionDistance\"] = big_df[\"CompetitionDistance\"].astype(int)\n",
    "big_df[\"CompetitionOpenSinceMonth\"] = big_df[\"CompetitionOpenSinceMonth\"].astype(int)\n",
    "big_df[\"CompetitionOpenSinceYear\"] = big_df[\"CompetitionOpenSinceYear\"].astype(int)\n",
    "\n",
    "big_df[\"Promo2SinceWeek\"] = big_df[\"Promo2SinceWeek\"].astype(int)\n",
    "big_df[\"Promo2SinceYear\"] = big_df[\"Promo2SinceYear\"].astype(int)\n",
    "\n",
    "#big_df[\"promo2week_bool\"] = big_df[\"Promo2SinceWeek\"] == big_df[\"Promo2SinceWeek\"].isna()\n",
    "\n",
    "store_type_dict = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3}\n",
    "big_df[\"StoreType\"] = big_df[\"StoreType\"].map(store_type_dict)\n",
    "\n",
    "assortment_dict = {\"a\": 0, \"b\": 1, \"c\": 2}\n",
    "big_df[\"Assortment\"] = big_df[\"Assortment\"].map(assortment_dict)\n",
    "\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(big_df[\"PromoInterval\"].unique()) # im 3-Monats Intervall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big_df[\"first_promo_month\"] = big_df[\"PromoInterval\"].str.split(\",\").str[0]\n",
    "\n",
    "big_df[\"jan\"] = 0\n",
    "big_df[\"feb\"] = 0\n",
    "big_df[\"mar\"] = 0\n",
    "big_df[\"apr\"] = 0\n",
    "big_df[\"may\"] = 0\n",
    "big_df[\"jun\"] = 0\n",
    "big_df[\"jul\"] = 0\n",
    "big_df[\"aug\"] = 0\n",
    "big_df[\"sep\"] = 0\n",
    "big_df[\"oct\"] = 0\n",
    "big_df[\"nov\"] = 0\n",
    "big_df[\"dec\"] = 0\n",
    "\n",
    "for index, row in big_df.iterrows():\n",
    "    start = row[\"PromoInterval\"].split(\",\")[0]\n",
    "    match start:\n",
    "        case \"Jan\":\n",
    "            big_df.at[index, \"jan\"] = 1\n",
    "            big_df.at[index, \"apr\"] = 1\n",
    "            big_df.at[index, \"jul\"] = 1\n",
    "            big_df.at[index, \"oct\"] = 1\n",
    "\n",
    "        case \"Feb\":\n",
    "            big_df.at[index, \"feb\"] = 1\n",
    "            big_df.at[index, \"may\"] = 1\n",
    "            big_df.at[index, \"aug\"] = 1\n",
    "            big_df.at[index, \"nov\"] = 1\n",
    "\n",
    "        case \"Mar\":\n",
    "            big_df.at[index, \"mar\"] = 1\n",
    "            big_df.at[index, \"jun\"] = 1\n",
    "            big_df.at[index, \"sep\"] = 1\n",
    "            big_df.at[index, \"dec\"] = 1\n",
    "\n",
    "\n",
    "big_df = big_df.drop(\"PromoInterval\", axis=1)\n",
    "big_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in big_df.columns:\n",
    "    col_n = re.sub(r'(?<!^)(?=[A-Z])', '_', col).lower()\n",
    "    big_df.rename(columns={col: col_n}, inplace=True)\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[\"state_holiday\"].unique()\n",
    "\n",
    "holiday_map = {\"0\": 0, \"a\": 1, \"b\": 2, \"c\": 3, 0: 4}\n",
    "\n",
    "big_df[\"state_holiday\"] = big_df[\"state_holiday\"].map(holiday_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = big_df.drop(\"sales\", axis=1)\n",
    "y = big_df[\"sales\"]\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X)\n",
    "x_scaled=scaler.transform(X)\n",
    "\n",
    "x_scaled_df = pd.DataFrame(x_scaled, columns=X.columns)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "border = 100000\n",
    "X_train = x_scaled_df[:border]\n",
    "y_train = y[:border]\n",
    "X_test = x_scaled_df[border:]\n",
    "y_test = y[border:]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "#print(\"Find the best parameters for the models\")\n",
    "#make(X_train, y_train)\n",
    "\n",
    "print(\"Load the best parameters from json file\")\n",
    "params_f = json.load(open(\"best_params.json\", \"r\"))\n",
    "\n",
    "\n",
    "def make_voting(X_train, y_train, X_test, y_test):\n",
    "    best_knn_params = params_f[\"knn\"]\n",
    "    best_tree_params = params_f[\"tree\"]\n",
    "    best_forest_params = params_f[\"forest\"]\n",
    "    best_svm_params = params_f[\"svm\"]\n",
    "    best_lin_reg_params = params_f[\"lin_reg\"]\n",
    "\n",
    "    knn = KNeighborsRegressor(n_neighbors=best_knn_params[\"kneighborsregressor__n_neighbors\"],\n",
    "                             weights=best_knn_params[\"kneighborsregressor__weights\"],\n",
    "                             algorithm=best_knn_params[\"kneighborsregressor__algorithm\"])\n",
    "    tree = DecisionTreeRegressor(max_depth=best_tree_params[\"decisiontreeregressor__max_depth\"],\n",
    "                                 min_samples_split=best_tree_params[\"decisiontreeregressor__min_samples_split\"],\n",
    "                                 min_samples_leaf=best_tree_params[\"decisiontreeregressor__min_samples_leaf\"])\n",
    "    forest = RandomForestRegressor(n_estimators=best_forest_params[\"randomforestregressor__n_estimators\"],\n",
    "                                   max_depth=best_forest_params[\"randomforestregressor__max_depth\"],\n",
    "                                   min_samples_split=best_forest_params[\"randomforestregressor__min_samples_split\"],\n",
    "                                   min_samples_leaf=best_forest_params[\"randomforestregressor__min_samples_leaf\"])\n",
    "    svm = SVR(kernel=best_svm_params[\"svr__kernel\"],\n",
    "             degree=best_svm_params[\"svr__degree\"],\n",
    "             C=best_svm_params[\"svr__C\"])\n",
    "    lin_reg = LinearRegression(fit_intercept=best_lin_reg_params[\"linearregression__fit_intercept\"])\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(\"knn done\")\n",
    "    tree.fit(X_train, y_train)\n",
    "    print(\"tree done\")\n",
    "    forest.fit(X_train, y_train)\n",
    "    print(\"forest done\")\n",
    "    svm.fit(X_train, y_train)\n",
    "    print(\"svm done\")\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    print(\"lin_reg done\")\n",
    "\n",
    "    voting = VotingRegressor(estimators=[(\"knn\", knn), (\"tree\", tree), (\"forest\", forest), (\"svm\", svm), (\"lin_reg\", lin_reg)], n_jobs=-1)\n",
    "\n",
    "    voting.fit(X_train, y_train)\n",
    "    print(\"voting done\")\n",
    "\n",
    "    error = mean_squared_error(y_test, voting.predict(X_test))\n",
    "    print(\"ERROR:\", error)\n",
    "\n",
    "    accuracy = voting.score(y_test, voting.predict(X_test))\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    return voting\n",
    "\n",
    "make_voting(X_train, y_train, X_test, y_test) # error: 276596.59625331557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1_h = 1000\n",
    "        self.l2_h = 3000\n",
    "        self.l3_h = 4000\n",
    "        self.l4_h = 1000\n",
    "        self.l5_h = 1\n",
    "\n",
    "        self.l1 = nn.Linear(X.shape[1], self.l1_h)\n",
    "        self.l2 = nn.Linear(self.l1_h, self.l2_h)\n",
    "        self.l3 = nn.Linear(self.l2_h, self.l3_h)\n",
    "        self.l4 = nn.Linear(self.l3_h, self.l4_h)\n",
    "        self.l5 = nn.Linear(self.l4_h, self.l5_h)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.l1(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.l2(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.l3(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.l4(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.l5(x)\n",
    "        #x = self.relu(x)\n",
    "\n",
    "        x = self.relu(self.l5(self.relu(self.l4(self.relu(self.l3(self.relu(self.l2(self.relu(self.l1(x))))))))))\n",
    "\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "torch.manual_seed(1234)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = Model()\n",
    "print(\"model initialized\")\n",
    "model.to(device)\n",
    "print(\"model on device\")\n",
    "optmizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "dataset = Dataset(X_train, y_train)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "schedular = torch.optim.lr_scheduler.CosineAnnealingLR(optmizer, T_max=20)\n",
    "\n",
    "epochs = 15\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "   for xb, yb in data_loader:\n",
    "       \n",
    "       xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "       y_hat = model(xb)\n",
    "       loss = loss_fn(y_hat, yb)\n",
    "       loss.backward()\n",
    "       optmizer.step()\n",
    "       optmizer.zero_grad()\n",
    "       schedular.step()\n",
    "   print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}\")\n",
    "   losses.append(loss.item())\n",
    "\n",
    "torch.save(model.state_dict(), \"modelGPU.pt\")\n",
    "\n",
    "plt.plot(range(epochs),losses, color=\"blue\")\n",
    "plt.legend([\"loss\"], loc=\"upper right\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available and if not, use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model instance and load state dict\n",
    "tm = Model()\n",
    "tm.load_state_dict(torch.load(\"modelGPU.pt\"))\n",
    "tm = tm.to(device)  # Move model to GPU\n",
    "\n",
    "count = 0\n",
    "total = len(y_test)\n",
    "\n",
    "for i in range(total):\n",
    "    if i % 1000 == 0:\n",
    "        print((i / total) * 100)\n",
    "\n",
    "    # Move test data to GPU before making predictions\n",
    "    X_test_tensor = torch.tensor(X_test.iloc[i], dtype=torch.float32).to(device)\n",
    "    y_hat = tm(X_test_tensor)\n",
    "\n",
    "    # Move y_hat back to CPU for comparison with y_test\n",
    "    y_hat = y_hat.to(\"cpu\")\n",
    "\n",
    "    if torch.argmax(y_hat).item() == y_test.iloc[i]:\n",
    "        count += 1\n",
    "\n",
    "print((count / total) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa22b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
