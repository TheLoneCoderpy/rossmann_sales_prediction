{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeitsschritte\n",
    "* Daten laden +\n",
    "* EDA +\n",
    "* Cleaning +\n",
    "* Feature Engineering\n",
    "* Modelle bauen (ML und NN)\n",
    "* Hyperparam. tuning für besten Modelle\n",
    "* Ensemble für Modelle\n",
    "* Feature Importance / Permutation Importance\n",
    "* Submission auf kaggle (+ Screenshot)\n",
    "* 2 geile Grafiken\n",
    "* je Kapitel eine Zusammenfassung\n",
    "\n",
    "Siehe itslearning Aufgabe\n",
    "\n",
    "# Mies viele Kommentare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from best_params import make\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "# from cuml.ensemble import RandomForestRegressor\n",
    "# from cuml.neighbors import KNeighborsRegressor\n",
    "# from cuml.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "store_df = pd.read_csv(\"store.csv\")\n",
    "\n",
    "big_df = df.merge(store_df, on=\"Store\", how=\"left\")\n",
    "store_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot 1: Sales by Day of Week\n",
    "s_b_d = big_df[\"Sales\"].groupby(big_df[\"DayOfWeek\"]).sum()\n",
    "axs[0, 0].plot(s_b_d)\n",
    "axs[0, 0].set_title('Sales by Day of Week')\n",
    "\n",
    "# Plot 2: Scatter plot of Customers vs Sales\n",
    "axs[0, 1].scatter(big_df[\"Customers\"], big_df[\"Sales\"])\n",
    "axs[0, 1].set_title('Customers vs Sales')\n",
    "\n",
    "# Plot 3: Sales by Store Type\n",
    "sales_by_storeType = big_df[\"Sales\"].groupby(big_df[\"StoreType\"]).sum()\n",
    "axs[1, 0].plot(sales_by_storeType)\n",
    "axs[1, 0].set_title('Sales by Store Type')\n",
    "\n",
    "# Plot 4: Amount of Stores by Type\n",
    "amt_stores_by_type = big_df[\"StoreType\"].value_counts()\n",
    "axs[1, 1].bar([\"a\", \"b\", \"c\", \"d\"], amt_stores_by_type)\n",
    "axs[1, 1].set_title('Amount of Stores by Type')\n",
    "\n",
    "# mean sales per customer\n",
    "sales_per_customer = big_df[\"Sales\"] / big_df[\"Customers\"]\n",
    "big_df[\"sales_per_customer\"] = sales_per_customer\n",
    "\n",
    "m_sales_customer_by_st = big_df[\"sales_per_customer\"].groupby(big_df[\"StoreType\"]).sum().values.tolist()\n",
    "axs[2, 0].plot(big_df[\"StoreType\"].unique(), m_sales_customer_by_st)\n",
    "axs[2, 0].set_title(\"mean_sales_p_cust_by_stoTyp\")\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datacleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[\"CompetitionDistance\"] = big_df[\"CompetitionDistance\"].fillna(0)\n",
    "\n",
    "comp_zeros = big_df[\"CompetitionDistance\"] == 0\n",
    "\n",
    "big_df[\"CompetitionOpenSinceMonth\"][comp_zeros] = 0\n",
    "big_df[\"CompetitionOpenSinceYear\"][comp_zeros] = 0\n",
    "\n",
    "big_df = big_df.dropna(axis=0)\n",
    "big_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big_df[\"year\"] = big_df[\"Date\"].str.split(\"-\").str[0].astype(int)\n",
    "#big_df[\"month\"] = big_df[\"Date\"].str.split(\"-\").str[1].astype(int)\n",
    "#big_df[\"day\"] = big_df[\"Date\"].str.split(\"-\").str[2].astype(int)\n",
    "\n",
    "to_drop = [\"Date\", \"Store\"]\n",
    "\n",
    "big_df = big_df.drop(to_drop, axis=1)\n",
    "\n",
    "big_df[\"CompetitionDistance\"] = big_df[\"CompetitionDistance\"].astype(int)\n",
    "big_df[\"CompetitionOpenSinceMonth\"] = big_df[\"CompetitionOpenSinceMonth\"].astype(int)\n",
    "big_df[\"CompetitionOpenSinceYear\"] = big_df[\"CompetitionOpenSinceYear\"].astype(int)\n",
    "\n",
    "big_df[\"Promo2SinceWeek\"] = big_df[\"Promo2SinceWeek\"].astype(int)\n",
    "big_df[\"Promo2SinceYear\"] = big_df[\"Promo2SinceYear\"].astype(int)\n",
    "\n",
    "#big_df[\"promo2week_bool\"] = big_df[\"Promo2SinceWeek\"] == big_df[\"Promo2SinceWeek\"].isna()\n",
    "\n",
    "store_type_dict = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3}\n",
    "big_df[\"StoreType\"] = big_df[\"StoreType\"].map(store_type_dict)\n",
    "\n",
    "assortment_dict = {\"a\": 0, \"b\": 1, \"c\": 2}\n",
    "big_df[\"Assortment\"] = big_df[\"Assortment\"].map(assortment_dict)\n",
    "\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(big_df[\"PromoInterval\"].unique()) # im 3-Monats Intervall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big_df[\"first_promo_month\"] = big_df[\"PromoInterval\"].str.split(\",\").str[0]\n",
    "\n",
    "big_df[\"jan\"] = 0\n",
    "big_df[\"feb\"] = 0\n",
    "big_df[\"mar\"] = 0\n",
    "big_df[\"apr\"] = 0\n",
    "big_df[\"may\"] = 0\n",
    "big_df[\"jun\"] = 0\n",
    "big_df[\"jul\"] = 0\n",
    "big_df[\"aug\"] = 0\n",
    "big_df[\"sep\"] = 0\n",
    "big_df[\"oct\"] = 0\n",
    "big_df[\"nov\"] = 0\n",
    "big_df[\"dec\"] = 0\n",
    "\n",
    "for index, row in big_df.iterrows():\n",
    "    start = row[\"PromoInterval\"].split(\",\")[0]\n",
    "    match start:\n",
    "        case \"Jan\":\n",
    "            big_df.at[index, \"jan\"] = 1\n",
    "            big_df.at[index, \"apr\"] = 1\n",
    "            big_df.at[index, \"jul\"] = 1\n",
    "            big_df.at[index, \"oct\"] = 1\n",
    "\n",
    "        case \"Feb\":\n",
    "            big_df.at[index, \"feb\"] = 1\n",
    "            big_df.at[index, \"may\"] = 1\n",
    "            big_df.at[index, \"aug\"] = 1\n",
    "            big_df.at[index, \"nov\"] = 1\n",
    "\n",
    "        case \"Mar\":\n",
    "            big_df.at[index, \"mar\"] = 1\n",
    "            big_df.at[index, \"jun\"] = 1\n",
    "            big_df.at[index, \"sep\"] = 1\n",
    "            big_df.at[index, \"dec\"] = 1\n",
    "\n",
    "\n",
    "big_df = big_df.drop(\"PromoInterval\", axis=1)\n",
    "big_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in big_df.columns:\n",
    "    col_n = re.sub(r'(?<!^)(?=[A-Z])', '_', col).lower()\n",
    "    big_df.rename(columns={col: col_n}, inplace=True)\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df[\"state_holiday\"].unique()\n",
    "\n",
    "holiday_map = {\"0\": 0, \"a\": 1, \"b\": 2, \"c\": 3, 0: 4}\n",
    "\n",
    "big_df[\"state_holiday\"] = big_df[\"state_holiday\"].map(holiday_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = big_df.drop(\"sales\", axis=1)\n",
    "y = big_df[\"sales\"]\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X)\n",
    "x_scaled=scaler.transform(X)\n",
    "\n",
    "x_scaled_df = pd.DataFrame(x_scaled, columns=X.columns)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "border = 100000\n",
    "X_train = x_scaled_df[:border]\n",
    "y_train = y[:border]\n",
    "X_test = x_scaled_df[border:]\n",
    "y_test = y[border:]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "#print(\"Find the best parameters for the models\")\n",
    "#make(X_train, y_train)\n",
    "\n",
    "print(\"Load the best parameters from json file\")\n",
    "params_f = json.load(open(\"best_params.json\", \"r\"))\n",
    "\n",
    "\n",
    "def make_voting(X_train, y_train, X_test, y_test):\n",
    "    best_knn_params = params_f[\"knn\"]\n",
    "    best_tree_params = params_f[\"tree\"]\n",
    "    best_forest_params = params_f[\"forest\"]\n",
    "    best_svm_params = params_f[\"svm\"]\n",
    "    best_lin_reg_params = params_f[\"lin_reg\"]\n",
    "\n",
    "    knn = KNeighborsRegressor(n_neighbors=best_knn_params[\"kneighborsregressor__n_neighbors\"],\n",
    "                             weights=best_knn_params[\"kneighborsregressor__weights\"],\n",
    "                             algorithm=best_knn_params[\"kneighborsregressor__algorithm\"])\n",
    "    tree = DecisionTreeRegressor(max_depth=best_tree_params[\"decisiontreeregressor__max_depth\"],\n",
    "                                 min_samples_split=best_tree_params[\"decisiontreeregressor__min_samples_split\"],\n",
    "                                 min_samples_leaf=best_tree_params[\"decisiontreeregressor__min_samples_leaf\"])\n",
    "    forest = RandomForestRegressor(n_estimators=best_forest_params[\"randomforestregressor__n_estimators\"],\n",
    "                                   max_depth=best_forest_params[\"randomforestregressor__max_depth\"],\n",
    "                                   min_samples_split=best_forest_params[\"randomforestregressor__min_samples_split\"],\n",
    "                                   min_samples_leaf=best_forest_params[\"randomforestregressor__min_samples_leaf\"])\n",
    "    svm = SVR(kernel=best_svm_params[\"svr__kernel\"],\n",
    "             degree=best_svm_params[\"svr__degree\"],\n",
    "             C=best_svm_params[\"svr__C\"])\n",
    "    lin_reg = LinearRegression(fit_intercept=best_lin_reg_params[\"linearregression__fit_intercept\"])\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(\"knn done\")\n",
    "    tree.fit(X_train, y_train)\n",
    "    print(\"tree done\")\n",
    "    forest.fit(X_train, y_train)\n",
    "    print(\"forest done\")\n",
    "    svm.fit(X_train, y_train)\n",
    "    print(\"svm done\")\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    print(\"lin_reg done\")\n",
    "\n",
    "    voting = VotingRegressor(estimators=[(\"knn\", knn), (\"tree\", tree), (\"forest\", forest), (\"svm\", svm), (\"lin_reg\", lin_reg)], n_jobs=-1)\n",
    "\n",
    "    voting.fit(X_train, y_train)\n",
    "    print(\"voting done\")\n",
    "\n",
    "    error = mean_squared_error(y_test, voting.predict(X_test))\n",
    "    print(\"ERROR:\", error)\n",
    "\n",
    "    accuracy = voting.score(y_test, voting.predict(X_test))\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    return voting\n",
    "\n",
    "# make_voting(X_train, y_train, X_test, y_test) # error: 276596.59625331557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1_h = 1000\n",
    "        self.l2_h = 3000\n",
    "        self.l3_h = 4000\n",
    "        self.l4_h = 1000\n",
    "        self.l5_h = 1\n",
    "\n",
    "        self.l1 = nn.Linear(X.shape[1], self.l1_h)\n",
    "        self.l2 = nn.Linear(self.l1_h, self.l2_h)\n",
    "        self.l3 = nn.Linear(self.l2_h, self.l3_h)\n",
    "        self.l4 = nn.Linear(self.l3_h, self.l4_h)\n",
    "        self.l5 = nn.Linear(self.l4_h, self.l5_h)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.l1(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.l2(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.l3(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.l4(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.l5(x)\n",
    "        #x = self.relu(x)\n",
    "\n",
    "        x = self.relu(self.l5(self.relu(self.l4(self.relu(self.l3(self.relu(self.l2(self.relu(self.l1(x))))))))))\n",
    "\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "torch.manual_seed(1234)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = Model()\n",
    "print(\"model initialized\")\n",
    "model.to(device)\n",
    "print(\"model on device\")\n",
    "optmizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "dataset = Dataset(X_train, y_train)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "schedular = torch.optim.lr_scheduler.CosineAnnealingLR(optmizer, T_max=20)\n",
    "\n",
    "epochs = 15\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "   for xb, yb in data_loader:\n",
    "       \n",
    "       xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "       y_hat = model(xb)\n",
    "       y_hat = y_hat.squeeze()  # remove dimensions of size 1 from the output\n",
    "       loss = loss_fn(y_hat, yb)\n",
    "       loss.backward()\n",
    "       optmizer.step()\n",
    "       optmizer.zero_grad()\n",
    "       schedular.step()\n",
    "   print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}\")\n",
    "   losses.append(loss.item())\n",
    "\n",
    "torch.save(model.state_dict(), \"modelGPU.pt\")\n",
    "\n",
    "plt.plot(range(epochs),losses, color=\"blue\")\n",
    "plt.legend([\"loss\"], loc=\"upper right\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5937889674009856\n",
      "1.1875779348019713\n",
      "1.781366902202957\n",
      "2.3751558696039425\n",
      "2.9689448370049285\n",
      "3.562733804405914\n",
      "4.1565227718069\n",
      "4.750311739207885\n",
      "5.344100706608871\n",
      "5.937889674009857\n",
      "6.531678641410843\n",
      "7.125467608811828\n",
      "7.719256576212814\n",
      "8.3130455436138\n",
      "8.906834511014786\n",
      "9.50062347841577\n",
      "10.094412445816756\n",
      "10.688201413217742\n",
      "11.281990380618728\n",
      "11.875779348019714\n",
      "12.4695683154207\n",
      "13.063357282821686\n",
      "13.65714625022267\n",
      "14.250935217623656\n",
      "14.84472418502464\n",
      "15.438513152425628\n",
      "16.032302119826614\n",
      "16.6260910872276\n",
      "17.219880054628586\n",
      "17.813669022029572\n",
      "18.407457989430558\n",
      "19.00124695683154\n",
      "19.59503592423253\n",
      "20.188824891633512\n",
      "20.782613859034498\n",
      "21.376402826435484\n",
      "21.97019179383647\n",
      "22.563980761237456\n",
      "23.157769728638442\n",
      "23.751558696039428\n",
      "24.345347663440414\n",
      "24.9391366308414\n",
      "25.532925598242386\n",
      "26.126714565643372\n",
      "26.720503533044354\n",
      "27.31429250044534\n",
      "27.90808146784633\n",
      "28.501870435247312\n",
      "29.095659402648298\n",
      "29.68944837004928\n",
      "30.28323733745027\n",
      "30.877026304851256\n",
      "31.47081527225224\n",
      "32.06460423965323\n",
      "32.65839320705422\n",
      "33.2521821744552\n",
      "33.84597114185618\n",
      "34.43976010925717\n",
      "35.033549076658154\n",
      "35.627338044059144\n",
      "36.221127011460126\n",
      "36.814915978861116\n",
      "37.4087049462621\n",
      "38.00249391366308\n",
      "38.59628288106407\n",
      "39.19007184846506\n",
      "39.78386081586604\n",
      "40.377649783267024\n",
      "40.97143875066801\n",
      "41.565227718068996\n",
      "42.159016685469986\n",
      "42.75280565287097\n",
      "43.34659462027196\n",
      "43.94038358767294\n",
      "44.53417255507392\n",
      "45.12796152247491\n",
      "45.7217504898759\n",
      "46.315539457276884\n",
      "46.909328424677874\n",
      "47.503117392078856\n",
      "48.09690635947984\n",
      "48.69069532688083\n",
      "49.28448429428181\n",
      "49.8782732616828\n",
      "50.47206222908378\n",
      "51.06585119648477\n",
      "51.659640163885754\n",
      "52.253429131286744\n",
      "52.84721809868773\n",
      "53.44100706608871\n",
      "54.0347960334897\n",
      "54.62858500089068\n",
      "55.22237396829167\n",
      "55.81616293569266\n",
      "56.409951903093635\n",
      "57.003740870494624\n",
      "57.597529837895614\n",
      "58.191318805296596\n",
      "58.785107772697586\n",
      "59.37889674009856\n",
      "59.97268570749955\n",
      "60.56647467490054\n",
      "61.16026364230153\n",
      "61.75405260970251\n",
      "62.3478415771035\n",
      "62.94163054450448\n",
      "63.53541951190547\n",
      "64.12920847930646\n",
      "64.72299744670744\n",
      "65.31678641410844\n",
      "65.9105753815094\n",
      "66.5043643489104\n",
      "67.09815331631138\n",
      "67.69194228371236\n",
      "68.28573125111336\n",
      "68.87952021851434\n",
      "69.47330918591533\n",
      "70.06709815331631\n",
      "70.66088712071729\n",
      "71.25467608811829\n",
      "71.84846505551927\n",
      "72.44225402292025\n",
      "73.03604299032123\n",
      "73.62983195772223\n",
      "74.22362092512321\n",
      "74.8174098925242\n",
      "75.4111988599252\n",
      "76.00498782732616\n",
      "76.59877679472716\n",
      "77.19256576212814\n",
      "77.78635472952912\n",
      "78.38014369693012\n",
      "78.97393266433109\n",
      "79.56772163173208\n",
      "80.16151059913307\n",
      "80.75529956653405\n",
      "81.34908853393505\n",
      "81.94287750133601\n",
      "82.53666646873701\n",
      "83.13045543613799\n",
      "83.72424440353899\n",
      "84.31803337093997\n",
      "84.91182233834095\n",
      "85.50561130574194\n",
      "86.09940027314292\n",
      "86.69318924054392\n",
      "87.2869782079449\n",
      "87.88076717534588\n",
      "88.47455614274686\n",
      "89.06834511014785\n",
      "89.66213407754884\n",
      "90.25592304494982\n",
      "90.8497120123508\n",
      "91.4435009797518\n",
      "92.03728994715277\n",
      "92.63107891455377\n",
      "93.22486788195475\n",
      "93.81865684935575\n",
      "94.41244581675673\n",
      "95.00623478415771\n",
      "95.6000237515587\n",
      "96.19381271895968\n",
      "96.78760168636067\n",
      "97.38139065376166\n",
      "97.97517962116264\n",
      "98.56896858856362\n",
      "99.1627575559646\n",
      "99.7565465233656\n",
      "Mean Absolute Error: 59.283716320814555\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and if not, use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model instance and load state dict\n",
    "tm = Model()\n",
    "tm.load_state_dict(torch.load(\"modelGPU.pt\"))\n",
    "tm = tm.to(device)  # Move model to GPU\n",
    "\n",
    "count = 0\n",
    "total = len(y_test)\n",
    "total_diff = 0\n",
    "\n",
    "for i in range(total):\n",
    "    if i % 1000 == 0:\n",
    "        print((i / total) * 100)\n",
    "\n",
    "    # Move test data to GPU before making predictions\n",
    "    X_test_tensor = torch.tensor(X_test.iloc[i].values, dtype=torch.float32).to(device)\n",
    "    y_hat = tm(X_test_tensor)  # Use tm to make predictions\n",
    "\n",
    "    # Move y_hat back to CPU for comparison with y_test\n",
    "    y_hat = y_hat.to(\"cpu\")\n",
    "\n",
    "    # Calculate the absolute difference between y_hat and y_test\n",
    "    diff = torch.abs(y_hat - y_test.iloc[i])\n",
    "    total_diff += diff.item()\n",
    "\n",
    "# Calculate the Mean Absolute Error\n",
    "mae = total_diff / total\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa22b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
